vllm using port  41869
[W512 10:53:21.046720654 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:186 (function operator())
INFO 05-12 10:53:32 [__init__.py:248] Automatically detected platform xpu.
WARNING 05-12 10:53:33 [_logger.py:68] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 05-12 10:53:49 [config.py:752] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
WARNING 05-12 10:53:49 [_logger.py:68] device type=xpu is not supported by the V1 Engine. Falling back to V0. 
INFO 05-12 10:53:49 [config.py:1815] Defaulting to use mp for distributed inference
INFO 05-12 10:53:49 [config.py:1849] Disabled the custom all-reduce kernel because it is not supported on current platform.
ERROR 05-12 10:53:49 [xpu.py:104] Both start methods (spawn and fork) have issue on XPU if you use mp backend, setting it to ray instead.
INFO 05-12 10:53:49 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.dev562+gc44c384b1) with config: model='/scratch/group/hprc/llama-models/llama-3_3-70B', speculative_config=None, tokenizer='/scratch/group/hprc/llama-models/llama-3_3-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=xpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/scratch/group/hprc/llama-models/llama-3_3-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
WARNING 05-12 10:53:50 [_logger.py:68] No existing RAY instance detected. A new instance will be launched with current node resources.
2025-05-12 10:53:54,499	INFO worker.py:1888 -- Started a local Ray instance.
INFO 05-12 10:53:58 [ray_utils.py:335] No current placement group found. Creating a new placement group.
INFO 05-12 10:54:04 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(pid=2889362)[0m [W512 10:54:08.705305317 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
[36m(pid=2889362)[0m   Overriding a previously registered kernel for the same operator and the same dispatch key
[36m(pid=2889362)[0m   operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
[36m(pid=2889362)[0m     registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
[36m(pid=2889362)[0m   dispatch key: XPU
[36m(pid=2889362)[0m   previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
[36m(pid=2889362)[0m        new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:186 (function operator())
[36m(pid=2889362)[0m INFO 05-12 10:54:10 [__init__.py:248] Automatically detected platform xpu.
[36m(pid=2889362)[0m WARNING 05-12 10:54:11 [_logger.py:68] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 05-12 10:54:12 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
INFO 05-12 10:54:12 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['VLLM_PORT', 'LD_LIBRARY_PATH', 'VLLM_USE_V1']
INFO 05-12 10:54:12 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/u.ks124812/.config/vllm/ray_non_carry_over_env_vars.json file
INFO 05-12 10:54:12 [xpu.py:35] Cannot use None backend on XPU.
INFO 05-12 10:54:12 [xpu.py:36] Using IPEX attention backend.
[36m(RayWorkerWrapper pid=2889363)[0m INFO 05-12 10:54:12 [xpu.py:35] Cannot use None backend on XPU.
[36m(RayWorkerWrapper pid=2889363)[0m INFO 05-12 10:54:12 [xpu.py:36] Using IPEX attention backend.
INFO 05-12 10:54:13 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_67cb7a9e'), local_subscribe_addr='ipc:///tmp/job.1108314/d8176e15-3133-4e72-a6fc-7e96e6b6c433', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 05-12 10:54:13 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
2025:05:12-10:54:13:(2887903) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)
2025:05:12-10:54:13:(2887903) |CCL_WARN| value of CCL_LOCAL_RANK changed to be 0 (default:-1)
2025:05:12-10:54:13:(2887903) |CCL_WARN| value of CCL_LOCAL_SIZE changed to be 4 (default:-1)
2025:05:12-10:54:13:(2887903) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be none (default:hydra)
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:54:14:(2887903) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode
[36m(RayWorkerWrapper pid=2889363)[0m INFO 05-12 10:54:13 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2889363)[0m 2025:05:12-10:54:14:(2889363) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode
Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:05<02:48,  5.80s/it]
Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:10<02:19,  5.00s/it]
Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:14<02:03,  4.57s/it]
Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:17<01:40,  3.85s/it]
Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:19<01:26,  3.46s/it]
Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:22<01:15,  3.13s/it]
Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:24<01:04,  2.80s/it]
Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:26<00:56,  2.59s/it]
Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:28<00:52,  2.51s/it]
Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:31<00:49,  2.48s/it]
Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:34<00:49,  2.62s/it]
Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:36<00:46,  2.57s/it]
Loading safetensors checkpoint shards:  43% Completed | 13/30 [00:40<00:49,  2.94s/it]
Loading safetensors checkpoint shards:  47% Completed | 14/30 [00:43<00:49,  3.08s/it]
Loading safetensors checkpoint shards:  50% Completed | 15/30 [00:47<00:46,  3.10s/it]
Loading safetensors checkpoint shards:  53% Completed | 16/30 [00:50<00:45,  3.27s/it]
Loading safetensors checkpoint shards:  57% Completed | 17/30 [00:55<00:46,  3.61s/it]
Loading safetensors checkpoint shards:  60% Completed | 18/30 [00:57<00:38,  3.17s/it]
Loading safetensors checkpoint shards:  63% Completed | 19/30 [00:59<00:31,  2.88s/it]
Loading safetensors checkpoint shards:  67% Completed | 20/30 [01:05<00:38,  3.84s/it]
Loading safetensors checkpoint shards:  70% Completed | 21/30 [01:08<00:31,  3.55s/it]
Loading safetensors checkpoint shards:  73% Completed | 22/30 [01:10<00:24,  3.11s/it]
Loading safetensors checkpoint shards:  77% Completed | 23/30 [01:12<00:20,  2.89s/it]
Loading safetensors checkpoint shards:  80% Completed | 24/30 [01:16<00:19,  3.18s/it]
Loading safetensors checkpoint shards:  83% Completed | 25/30 [01:18<00:14,  2.82s/it]
Loading safetensors checkpoint shards:  87% Completed | 26/30 [01:21<00:11,  2.75s/it]
Loading safetensors checkpoint shards:  90% Completed | 27/30 [01:23<00:07,  2.60s/it]
Loading safetensors checkpoint shards:  93% Completed | 28/30 [01:26<00:05,  2.66s/it]
Loading safetensors checkpoint shards:  97% Completed | 29/30 [01:29<00:02,  2.69s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [01:32<00:00,  2.78s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [01:32<00:00,  3.07s/it]

INFO 05-12 10:55:51 [default_loader.py:278] Loading weights took 92.16 seconds
WARNING 05-12 10:55:51 [_logger.py:68] Pin memory is not supported on XPU.
INFO 05-12 10:55:51 [xpu_model_runner.py:414] Loading model weights took 32.8894 GiB
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-10:56:19:(2887903) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode
[36m(RayWorkerWrapper pid=2889365)[0m INFO 05-12 10:56:17 [default_loader.py:278] Loading weights took 118.03 seconds
[36m(RayWorkerWrapper pid=2889365)[0m WARNING 05-12 10:56:17 [_logger.py:68] Pin memory is not supported on XPU.
[36m(pid=2889365)[0m INFO 05-12 10:54:10 [__init__.py:248] Automatically detected platform xpu.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=2889364)[0m WARNING 05-12 10:54:11 [_logger.py:68] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=2889365)[0m INFO 05-12 10:54:12 [xpu.py:35] Cannot use None backend on XPU.[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2889365)[0m INFO 05-12 10:54:12 [xpu.py:36] Using IPEX attention backend.[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2889365)[0m INFO 05-12 10:54:13 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2889365)[0m 2025:05:12-10:54:14:(2889365) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=2889365)[0m 2025:05:12-10:54:14:(2889365) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2889365)[0m INFO 05-12 10:56:17 [xpu_model_runner.py:414] Loading model weights took 32.8894 GiB
INFO 05-12 10:56:25 [executor_base.py:112] # xpu blocks: 8446, # CPU blocks: 3276
INFO 05-12 10:56:25 [executor_base.py:117] Maximum concurrency for 1024 tokens per request: 131.97x
INFO 05-12 10:56:25 [llm_engine.py:435] init engine (profile, create kv cache, warmup model) took 8.04 seconds
 * Serving Flask app 'app'
 * Debug mode: off
2025-05-12 10:56:25,821 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:1025
 * Running on http://127.0.0.1:1025
2025-05-12 10:56:25,821 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
my input:   is not defined NameError: name hello is not defined
max length:  256
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 32.91it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]2025-05-12 10:57:14,141 - werkzeug - INFO - 10.71.8.1 - - [12/May/2025 10:57:14] "POST /infer HTTP/1.1" 200 -
Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.07s/it, est. speed input: 0.34 toks/s, output: 7.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.07s/it, est. speed input: 0.34 toks/s, output: 7.30 toks/s]
[RequestOutput(request_id=0, prompt=' is not defined NameError: name hello is not defined', prompt_token_ids=[128000, 374, 539, 4613, 4076, 1480, 25, 836, 24748, 374, 539, 4613], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\n```\nfrom collections import defaultdict\n\nclass TrieNode:\n    def __init__(self):\n        self.children = defaultdict(TrieNode)\n        self.end_of_word = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        node = self.root\n        for ch in word:\n            node = node.children[ch]\n        node.end_of_word = True\n\n    def search(self, word):\n        node = self.root\n        for ch in word:\n            if ch not in node.children:\n                return False\n            node = node.children[ch]\n        return node.end_of_word\n\n    def starts_with(self, prefix):\n        node = self.root\n        for ch in prefix:\n            if ch not in node.children:\n                return False\n            node = node.children[ch]\n        return True\n\ndef hello():\n    trie = Trie()\n    trie.insert("apple")\n    trie.insert("app")\n    trie.insert("banana")\n    print(trie.search("apple"))  # Expected output: True\n    print(trie.search("app"))  # Expected output: True\n    print(trie.search("banana"))  # Expected output: True\n    print(trie.search("bananas")) ', token_ids=(198, 14196, 4077, 1527, 15661, 1179, 44008, 271, 1058, 63883, 1997, 512, 262, 711, 1328, 2381, 3889, 726, 997, 286, 659, 13664, 284, 44008, 4233, 7379, 1997, 340, 286, 659, 5183, 3659, 13843, 284, 3641, 271, 1058, 63883, 512, 262, 711, 1328, 2381, 3889, 726, 997, 286, 659, 12861, 284, 63883, 1997, 2892, 262, 711, 5774, 1214, 11, 3492, 997, 286, 2494, 284, 659, 12861, 198, 286, 369, 523, 304, 3492, 512, 310, 2494, 284, 2494, 13664, 58861, 933, 286, 2494, 5183, 3659, 13843, 284, 3082, 271, 262, 711, 2778, 1214, 11, 3492, 997, 286, 2494, 284, 659, 12861, 198, 286, 369, 523, 304, 3492, 512, 310, 422, 523, 539, 304, 2494, 13664, 512, 394, 471, 3641, 198, 310, 2494, 284, 2494, 13664, 58861, 933, 286, 471, 2494, 5183, 3659, 13843, 271, 262, 711, 8638, 6753, 1214, 11, 9436, 997, 286, 2494, 284, 659, 12861, 198, 286, 369, 523, 304, 9436, 512, 310, 422, 523, 539, 304, 2494, 13664, 512, 394, 471, 3641, 198, 310, 2494, 284, 2494, 13664, 58861, 933, 286, 471, 3082, 271, 755, 24748, 4019, 262, 60167, 284, 63883, 746, 262, 60167, 7175, 446, 23182, 1158, 262, 60167, 7175, 446, 680, 1158, 262, 60167, 7175, 446, 88847, 1158, 262, 1194, 7779, 648, 9472, 446, 23182, 2830, 220, 674, 32121, 2612, 25, 3082, 198, 262, 1194, 7779, 648, 9472, 446, 680, 2830, 220, 674, 32121, 2612, 25, 3082, 198, 262, 1194, 7779, 648, 9472, 446, 88847, 2830, 220, 674, 32121, 2612, 25, 3082, 198, 262, 1194, 7779, 648, 9472, 446, 6993, 26997, 2830, 220), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1747065429.1420135, last_token_time=1747065464.2431908, first_scheduled_time=1747065429.1727996, first_token_time=1747065429.6796355, time_in_queue=0.03078603744506836, finished_time=1747065464.2434142, scheduler_time=0.0314048525178805, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]
CompletionOutput(index=0, text='\n```\nfrom collections import defaultdict\n\nclass TrieNode:\n    def __init__(self):\n        self.children = defaultdict(TrieNode)\n        self.end_of_word = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        node = self.root\n        for ch in word:\n            node = node.children[ch]\n        node.end_of_word = True\n\n    def search(self, word):\n        node = self.root\n        for ch in word:\n            if ch not in node.children:\n                return False\n            node = node.children[ch]\n        return node.end_of_word\n\n    def starts_with(self, prefix):\n        node = self.root\n        for ch in prefix:\n            if ch not in node.children:\n                return False\n            node = node.children[ch]\n        return True\n\ndef hello():\n    trie = Trie()\n    trie.insert("apple")\n    trie.insert("app")\n    trie.insert("banana")\n    print(trie.search("apple"))  # Expected output: True\n    print(trie.search("app"))  # Expected output: True\n    print(trie.search("banana"))  # Expected output: True\n    print(trie.search("bananas")) ', token_ids=(198, 14196, 4077, 1527, 15661, 1179, 44008, 271, 1058, 63883, 1997, 512, 262, 711, 1328, 2381, 3889, 726, 997, 286, 659, 13664, 284, 44008, 4233, 7379, 1997, 340, 286, 659, 5183, 3659, 13843, 284, 3641, 271, 1058, 63883, 512, 262, 711, 1328, 2381, 3889, 726, 997, 286, 659, 12861, 284, 63883, 1997, 2892, 262, 711, 5774, 1214, 11, 3492, 997, 286, 2494, 284, 659, 12861, 198, 286, 369, 523, 304, 3492, 512, 310, 2494, 284, 2494, 13664, 58861, 933, 286, 2494, 5183, 3659, 13843, 284, 3082, 271, 262, 711, 2778, 1214, 11, 3492, 997, 286, 2494, 284, 659, 12861, 198, 286, 369, 523, 304, 3492, 512, 310, 422, 523, 539, 304, 2494, 13664, 512, 394, 471, 3641, 198, 310, 2494, 284, 2494, 13664, 58861, 933, 286, 471, 2494, 5183, 3659, 13843, 271, 262, 711, 8638, 6753, 1214, 11, 9436, 997, 286, 2494, 284, 659, 12861, 198, 286, 369, 523, 304, 9436, 512, 310, 422, 523, 539, 304, 2494, 13664, 512, 394, 471, 3641, 198, 310, 2494, 284, 2494, 13664, 58861, 933, 286, 471, 3082, 271, 755, 24748, 4019, 262, 60167, 284, 63883, 746, 262, 60167, 7175, 446, 23182, 1158, 262, 60167, 7175, 446, 680, 1158, 262, 60167, 7175, 446, 88847, 1158, 262, 1194, 7779, 648, 9472, 446, 23182, 2830, 220, 674, 32121, 2612, 25, 3082, 198, 262, 1194, 7779, 648, 9472, 446, 680, 2830, 220, 674, 32121, 2612, 25, 3082, 198, 262, 1194, 7779, 648, 9472, 446, 88847, 2830, 220, 674, 32121, 2612, 25, 3082, 198, 262, 1194, 7779, 648, 9472, 446, 6993, 26997, 2830, 220), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)
returning result:  
```
from collections import defaultdict

class TrieNode:
    def __init__(self):
        self.children = defaultdict(TrieNode)
        self.end_of_word = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word):
        node = self.root
        for ch in word:
            node = node.children[ch]
        node.end_of_word = True

    def search(self, word):
        node = self.root
        for ch in word:
            if ch not in node.children:
                return False
            node = node.children[ch]
        return node.end_of_word

    def starts_with(self, prefix):
        node = self.root
        for ch in prefix:
            if ch not in node.children:
                return False
            node = node.children[ch]
        return True

def hello():
    trie = Trie()
    trie.insert("apple")
    trie.insert("app")
    trie.insert("banana")
    print(trie.search("apple"))  # Expected output: True
    print(trie.search("app"))  # Expected output: True
    print(trie.search("banana"))  # Expected output: True
    print(trie.search("bananas")) 

2025-05-12 10:57:44,247 - werkzeug - INFO - 10.71.8.1 - - [12/May/2025 10:57:44] "POST /infer HTTP/1.1" 200 -
