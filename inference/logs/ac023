vllm using port  21496
[W512 12:59:02.293338017 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:186 (function operator())
INFO 05-12 12:59:10 [__init__.py:248] Automatically detected platform xpu.
WARNING 05-12 12:59:11 [_logger.py:68] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 05-12 12:59:25 [config.py:752] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
WARNING 05-12 12:59:25 [_logger.py:68] device type=xpu is not supported by the V1 Engine. Falling back to V0. 
INFO 05-12 12:59:25 [config.py:1815] Defaulting to use mp for distributed inference
INFO 05-12 12:59:25 [config.py:1849] Disabled the custom all-reduce kernel because it is not supported on current platform.
ERROR 05-12 12:59:25 [xpu.py:104] Both start methods (spawn and fork) have issue on XPU if you use mp backend, setting it to ray instead.
INFO 05-12 12:59:25 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.dev562+gc44c384b1) with config: model='/scratch/group/hprc/llama-models/llama-3_3-70B', speculative_config=None, tokenizer='/scratch/group/hprc/llama-models/llama-3_3-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=xpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/scratch/group/hprc/llama-models/llama-3_3-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
WARNING 05-12 12:59:26 [_logger.py:68] No existing RAY instance detected. A new instance will be launched with current node resources.
2025-05-12 12:59:29,983	INFO worker.py:1888 -- Started a local Ray instance.
INFO 05-12 12:59:33 [ray_utils.py:335] No current placement group found. Creating a new placement group.
INFO 05-12 12:59:38 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(pid=2961251)[0m [W512 12:59:43.112973904 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
[36m(pid=2961251)[0m   Overriding a previously registered kernel for the same operator and the same dispatch key
[36m(pid=2961251)[0m   operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
[36m(pid=2961251)[0m     registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
[36m(pid=2961251)[0m   dispatch key: XPU
[36m(pid=2961251)[0m   previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
[36m(pid=2961251)[0m        new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:186 (function operator())
[36m(pid=2961252)[0m INFO 05-12 12:59:44 [__init__.py:248] Automatically detected platform xpu.
[36m(pid=2961252)[0m WARNING 05-12 12:59:45 [_logger.py:68] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
INFO 05-12 12:59:46 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
INFO 05-12 12:59:46 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['VLLM_PORT', 'LD_LIBRARY_PATH', 'VLLM_USE_V1']
INFO 05-12 12:59:46 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/u.ks124812/.config/vllm/ray_non_carry_over_env_vars.json file
INFO 05-12 12:59:46 [xpu.py:35] Cannot use None backend on XPU.
INFO 05-12 12:59:46 [xpu.py:36] Using IPEX attention backend.
[36m(RayWorkerWrapper pid=2961252)[0m INFO 05-12 12:59:46 [xpu.py:35] Cannot use None backend on XPU.
[36m(RayWorkerWrapper pid=2961252)[0m INFO 05-12 12:59:46 [xpu.py:36] Using IPEX attention backend.
INFO 05-12 12:59:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e40abd5b'), local_subscribe_addr='ipc:///tmp/job.1108402/92eafac3-f0e9-40af-93cd-92c0a97bc7a3', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 05-12 12:59:48 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
2025:05:12-12:59:48:(2960053) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)
2025:05:12-12:59:48:(2960053) |CCL_WARN| value of CCL_LOCAL_RANK changed to be 0 (default:-1)
2025:05:12-12:59:48:(2960053) |CCL_WARN| value of CCL_LOCAL_SIZE changed to be 4 (default:-1)
2025:05:12-12:59:48:(2960053) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be none (default:hydra)
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-12:59:49:(2960053) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode
[36m(RayWorkerWrapper pid=2961252)[0m INFO 05-12 12:59:48 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(RayWorkerWrapper pid=2961252)[0m 2025:05:12-12:59:49:(2961252) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode
[36m(pid=2961257)[0m INFO 05-12 12:59:44 [__init__.py:248] Automatically detected platform xpu.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:06<02:55,  6.06s/it]
Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:11<02:34,  5.50s/it]
Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:16<02:21,  5.26s/it]
Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:21<02:13,  5.14s/it]
Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:24<01:52,  4.51s/it]
Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:27<01:39,  4.15s/it]
Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:31<01:27,  3.82s/it]
Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:33<01:14,  3.40s/it]
Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:36<01:09,  3.29s/it]
Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:40<01:10,  3.54s/it]
Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:44<01:08,  3.62s/it]
Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:46<00:58,  3.23s/it]
Loading safetensors checkpoint shards:  43% Completed | 13/30 [00:50<00:55,  3.25s/it]
Loading safetensors checkpoint shards:  47% Completed | 14/30 [00:52<00:46,  2.88s/it]
Loading safetensors checkpoint shards:  50% Completed | 15/30 [00:55<00:43,  2.93s/it]
Loading safetensors checkpoint shards:  53% Completed | 16/30 [00:57<00:38,  2.73s/it]
Loading safetensors checkpoint shards:  57% Completed | 17/30 [00:59<00:33,  2.60s/it]
Loading safetensors checkpoint shards:  60% Completed | 18/30 [01:01<00:28,  2.36s/it]
Loading safetensors checkpoint shards:  63% Completed | 19/30 [01:04<00:26,  2.45s/it]
Loading safetensors checkpoint shards:  67% Completed | 20/30 [01:07<00:26,  2.61s/it]
Loading safetensors checkpoint shards:  70% Completed | 21/30 [01:10<00:24,  2.74s/it]
Loading safetensors checkpoint shards:  73% Completed | 22/30 [01:12<00:20,  2.59s/it]
Loading safetensors checkpoint shards:  77% Completed | 23/30 [01:14<00:17,  2.55s/it]
Loading safetensors checkpoint shards:  80% Completed | 24/30 [01:18<00:16,  2.72s/it]
Loading safetensors checkpoint shards:  83% Completed | 25/30 [01:21<00:14,  2.83s/it]
Loading safetensors checkpoint shards:  87% Completed | 26/30 [01:23<00:10,  2.58s/it]
Loading safetensors checkpoint shards:  90% Completed | 27/30 [01:25<00:07,  2.41s/it]
Loading safetensors checkpoint shards:  93% Completed | 28/30 [01:28<00:05,  2.62s/it]
Loading safetensors checkpoint shards:  97% Completed | 29/30 [01:31<00:02,  2.67s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [01:34<00:00,  3.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [01:34<00:00,  3.16s/it]

INFO 05-12 13:01:28 [default_loader.py:278] Loading weights took 94.92 seconds
WARNING 05-12 13:01:28 [_logger.py:68] Pin memory is not supported on XPU.
INFO 05-12 13:01:28 [xpu_model_runner.py:414] Loading model weights took 32.8894 GiB
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:05:12-13:01:57:(2960053) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode
[36m(RayWorkerWrapper pid=2961249)[0m INFO 05-12 13:01:54 [default_loader.py:278] Loading weights took 120.76 seconds
[36m(RayWorkerWrapper pid=2961249)[0m WARNING 05-12 13:01:54 [_logger.py:68] Pin memory is not supported on XPU.
[36m(pid=2961257)[0m WARNING 05-12 12:59:45 [_logger.py:68] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=2961257)[0m INFO 05-12 12:59:46 [xpu.py:35] Cannot use None backend on XPU.[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2961257)[0m INFO 05-12 12:59:46 [xpu.py:36] Using IPEX attention backend.[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2961257)[0m INFO 05-12 12:59:48 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2961257)[0m 2025:05:12-12:59:49:(2961257) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=2961257)[0m 2025:05:12-12:59:49:(2961257) |CCL_WARN| pidfd is not supported, fallbacks to drmfd exchange mode[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2961252)[0m INFO 05-12 13:01:54 [xpu_model_runner.py:414] Loading model weights took 32.8894 GiB
INFO 05-12 13:02:04 [executor_base.py:112] # xpu blocks: 8446, # CPU blocks: 3276
INFO 05-12 13:02:04 [executor_base.py:117] Maximum concurrency for 1024 tokens per request: 131.97x
INFO 05-12 13:02:04 [llm_engine.py:435] init engine (profile, create kv cache, warmup model) took 10.15 seconds
 * Serving Flask app 'app'
 * Debug mode: off
2025-05-12 13:02:04,774 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:1025
 * Running on http://127.0.0.1:1025
2025-05-12 13:02:04,774 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
my input:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.



Question: what is the square root of 2
Helpful Answer:
max length:  512
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 44.21it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.47s/it, est. speed input: 8.50 toks/s, output: 6.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.47s/it, est. speed input: 8.50 toks/s, output: 6.95 toks/s]
[RequestOutput(request_id=0, prompt="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n\n\nQuestion: what is the square root of 2\nHelpful Answer:", prompt_token_ids=[128000, 10464, 279, 2768, 9863, 315, 2317, 311, 4320, 279, 3488, 520, 279, 842, 13, 1442, 499, 1541, 956, 1440, 279, 4320, 11, 1120, 2019, 430, 499, 1541, 956, 1440, 11, 1541, 956, 1456, 311, 1304, 709, 459, 4320, 2055, 14924, 25, 1148, 374, 279, 9518, 3789, 315, 220, 17, 198, 12978, 1285, 22559, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' The square root of 2, often represented by √2, is an irrational number that is approximately equal to 1.414. It cannot be expressed as a simple fraction and has an infinite number of decimal places.', token_ids=(578, 9518, 3789, 315, 220, 17, 11, 3629, 15609, 555, 122371, 17, 11, 374, 459, 61754, 1396, 430, 374, 13489, 6273, 311, 220, 16, 13, 17448, 13, 1102, 4250, 387, 13605, 439, 264, 4382, 19983, 323, 706, 459, 24746, 1396, 315, 12395, 7634, 13, 128009), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1747073177.177325, last_token_time=1747073183.6697242, first_scheduled_time=1747073177.200341, first_token_time=1747073177.7127814, time_in_queue=0.023015975952148438, finished_time=1747073183.669935, scheduler_time=0.007164254842791706, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]
CompletionOutput(index=0, text=' The square root of 2, often represented by √2, is an irrational number that is approximately equal to 1.414. It cannot be expressed as a simple fraction and has an infinite number of decimal places.', token_ids=(578, 9518, 3789, 315, 220, 17, 11, 3629, 15609, 555, 122371, 17, 11, 374, 459, 61754, 1396, 430, 374, 13489, 6273, 311, 220, 16, 13, 17448, 13, 1102, 4250, 387, 13605, 439, 264, 4382, 19983, 323, 706, 459, 24746, 1396, 315, 12395, 7634, 13, 128009), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)
returning result:   The square root of 2, often represented by √2, is an irrational number that is approximately equal to 1.414. It cannot be expressed as a simple fraction and has an infinite number of decimal places.

2025-05-12 13:06:23,675 - werkzeug - INFO - 10.71.8.1 - - [12/May/2025 13:06:23] "POST /infer HTTP/1.1" 200 -
